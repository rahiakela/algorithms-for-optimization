{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-derivatives-and-gradients.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/algorithms-for-optimization/blob/main/2-derivatives-and-gradients/1_derivatives_and_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# Derivatives and Gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8pQ2z2k5RBv"
      },
      "source": [
        "**Optimization is concerned with finding the design point that minimizes (or\r\n",
        "maximizes) an objective function.** Knowing how the value of a function changes\r\n",
        "as its input is varied is useful because it tells us in which direction we can move to improve on previous points. **The change in the value of the function is measured by the derivative in one dimension and the gradient in multiple dimensions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS7WHNiF5RWQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIeFXS0F0zww"
      },
      "source": [
        "from sympy import *"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6SMJNXu6Kjo"
      },
      "source": [
        "## Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJjpuxK6LsQ"
      },
      "source": [
        "The derivative $f^{'}(x)$ of a function $f$ of a single variable x is the rate at which the value of f changes at $x$. It is often visualized, using the tangent line to the graph of the function at $x$. The value of the derivative equals\r\n",
        "the slope of the tangent line.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/algorithms-for-optimization/tangent-line.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We can use the derivative to provide a linear approximation of the function\r\n",
        "near x:\r\n",
        "\r\n",
        "$$f(x + \\delta{x}) \\approx f(x) + f^{'}(x) \\delta{x}$$\r\n",
        "\r\n",
        "The derivative is the ratio between the change in $f$ and the change in $x$ at the point $x$:\r\n",
        "\r\n",
        "$$  f^{'}(x) = \\frac{\\delta{f(x)}}  {\\delta{x}} $$\r\n",
        "\r\n",
        "which is the change in $f(x)$ divided by the change in $x$ as the step becomes\r\n",
        "infinitesimally small.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/algorithms-for-optimization/step-differences.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The notation $f^{'}(x)$ can be attributed to Lagrange. We also use the notation created by Leibniz,\r\n",
        "\r\n",
        "$$  f^{'}(x) = \\frac{df(x)}  {dx} $$\r\n",
        "\r\n",
        "which emphasizes the fact that the derivative is the ratio of the change in $f$ to the change in $x$ at the point $x$.\r\n",
        "\r\n",
        "The limit equation defining the derivative can be presented in three different\r\n",
        "ways: \r\n",
        "- the forward difference, \r\n",
        "- the central difference, \r\n",
        "- and the backward difference. \r\n",
        "\r\n",
        "Each method uses an infinitely small step size $h$:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/algorithms-for-optimization/symbolic-differentiation.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "If $f$ can be represented symbolically, symbolic differentiation can often provide an exact analytic expression for $f^{'}$ by applying derivative rules from calculus. The analytic expression can then be evaluated at any point $x$.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tytAeuaA1eau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e15bf98-c779-4cd9-fabf-ebe70cbb3d04"
      },
      "source": [
        "# define x as a symbolic variable\r\n",
        "x = symbols(\"x\")\r\n",
        "\r\n",
        "# Define function\r\n",
        "f = x**2+x/2 - sin(x)/x\r\n",
        "\r\n",
        "# Calculating Derivative\r\n",
        "f_prime = diff(f, x)\r\n",
        "f_prime"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2*x + 1/2 - cos(x)/x + sin(x)/x**2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTtt_geoRdtB",
        "outputId": "6439a7ad-ff22-466b-fcb8-bf817517f7da"
      },
      "source": [
        "# calculation\r\n",
        "f_p = lambdify(x, f)\r\n",
        "\r\n",
        "# passing x=2 to the function\r\n",
        "f_p(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5453512865871595"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ql8TrXSAsq",
        "outputId": "e458469d-a7f8-4dbd-be08-0e0d0f3e1562"
      },
      "source": [
        "f_p(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27.691784854932628"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hGQKwIjJ0IX"
      },
      "source": [
        "Ref: https://www.askpython.com/python/examples/derivatives-in-python-sympy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py96INTlCXnT"
      },
      "source": [
        "## Derivatives in Multiple Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WeCWQcUCY8M"
      },
      "source": [
        "The gradient is the generalization of the derivative to multivariate functions. It captures the local slope of the function, allowing us to predict the effect of taking a small step from a point in any direction.\r\n",
        "\r\n",
        "$$w_1x_1 + · · · + w_nx_n = b$$\r\n",
        "\r\n",
        "for some vector $w$ and scalar $b$. A hyperplane has $n − 1$ dimensions.\r\n",
        "\r\n",
        "The gradient of f at x is written $\\Delta{f(x)}$ and is a vector. Each component of that vector is the partial derivative of $f$ with respect to that component:\r\n",
        "\r\n",
        ">>The partial derivative of a function with respect to a variable is the derivative assuming all other input variables are held constant. It is denoted $\\frac{\\partial \\mathbf{f}}{\\partial x}$.\r\n",
        "\r\n",
        "$$ \\Delta{f(x)} = \\left[ \r\n",
        "  \\frac{\\partial \\mathbf{f(x)}}{\\partial x_1}, \r\n",
        "  \\frac{\\partial \\mathbf{f(x)}}{\\partial x_2},\r\n",
        "  \\cdots,\r\n",
        "  \\frac{\\partial \\mathbf{f(x)}}{\\partial x_n} \r\n",
        "\\right] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CferKyH2RRCJ"
      },
      "source": [
        "## Numerical Differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeUJsJvYRToX"
      },
      "source": [
        ""
      ]
    }
  ]
}