{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-derivatives-and-gradients.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/algorithms-for-optimization/blob/main/2-derivatives-and-gradients/1_derivatives_and_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# Derivatives and Gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8pQ2z2k5RBv"
      },
      "source": [
        "**Optimization is concerned with finding the design point that minimizes (or\r\n",
        "maximizes) an objective function.** Knowing how the value of a function changes\r\n",
        "as its input is varied is useful because it tells us in which direction we can move to improve on previous points. **The change in the value of the function is measured by the derivative in one dimension and the gradient in multiple dimensions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS7WHNiF5RWQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIeFXS0F0zww"
      },
      "source": [
        "from sympy import *"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6SMJNXu6Kjo"
      },
      "source": [
        "## Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJjpuxK6LsQ"
      },
      "source": [
        "The derivative $f^{'}(x)$ of a function $f$ of a single variable x is the rate at which the value of f changes at $x$. It is often visualized, using the tangent line to the graph of the function at $x$. The value of the derivative equals\r\n",
        "the slope of the tangent line.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/algorithms-for-optimization/tangent-line.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We can use the derivative to provide a linear approximation of the function\r\n",
        "near x:\r\n",
        "\r\n",
        "$$f(x + \\delta{x}) \\approx f(x) + f^{'}(x) \\delta{x}$$\r\n",
        "\r\n",
        "The derivative is the ratio between the change in $f$ and the change in $x$ at the point $x$:\r\n",
        "\r\n",
        "$$  f^{'}(x) = \\frac{\\delta{f(x)}}  {\\delta{x}} $$\r\n",
        "\r\n",
        "which is the change in $f(x)$ divided by the change in $x$ as the step becomes\r\n",
        "infinitesimally small.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/algorithms-for-optimization/step-differences.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The notation $f^{'}(x)$ can be attributed to Lagrange. We also use the notation created by Leibniz,\r\n",
        "\r\n",
        "$$  f^{'}(x) = \\frac{df(x)}  {dx} $$\r\n",
        "\r\n",
        "which emphasizes the fact that the derivative is the ratio of the change in $f$ to the change in $x$ at the point $x$.\r\n",
        "\r\n",
        "The limit equation defining the derivative can be presented in three different\r\n",
        "ways: \r\n",
        "- the forward difference, \r\n",
        "- the central difference, \r\n",
        "- and the backward difference. \r\n",
        "\r\n",
        "Each method uses an infinitely small step size $h$:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/algorithms-for-optimization/symbolic-differentiation.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "If $f$ can be represented symbolically, symbolic differentiation can often provide an exact analytic expression for $f^{'}$ by applying derivative rules from calculus. The analytic expression can then be evaluated at any point $x$.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tytAeuaA1eau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22efa52-c7f5-45fd-8ae1-55e2453a177a"
      },
      "source": [
        "# define x as a symbolic variable\r\n",
        "x = symbols(\"x\")\r\n",
        "\r\n",
        "# Define function\r\n",
        "f = x**2+x/2 - sin(x)/x\r\n",
        "\r\n",
        "# Calculating Derivative\r\n",
        "diff(f, x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2*x + 1/2 - cos(x)/x + sin(x)/x**2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hGQKwIjJ0IX"
      },
      "source": [
        "Ref: https://www.askpython.com/python/examples/derivatives-in-python-sympy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py96INTlCXnT"
      },
      "source": [
        "## Derivatives in Multiple Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WeCWQcUCY8M"
      },
      "source": [
        ""
      ]
    }
  ]
}